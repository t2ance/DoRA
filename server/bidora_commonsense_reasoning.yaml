apiVersion: batch/v1
kind: Job
metadata:
  name: bidora-commonsense-reasoning
spec:
  template:
    spec:
      containers:
        - name: gpu-container
          image: continuumio/anaconda3
          command: [ "/bin/bash","-c" ]
          args: [
            "nvidia-smi;
             git clone https://github.com/t2ance/DoRA.git;
             cd DoRA;
             cd commonsense_reasoning;
             conda create -n dora_llama python=3.10;
             echo 'created!';
             conda init bash;
             source /opt/conda/etc/profile.d/conda.sh;
             conda activate dora_llama;
             echo 'activated!';
             pip install -r requirements.txt;
             echo 'installed!';
             bash ./download_data.sh;
             echo 'downloaded!';
             sh llama_7B_BiDora.sh 32 64 ./finetuned_result/bidora_r32 0;
             jupyter lab --ip=0.0.0.0 --port=8888 --NotebookApp.token=627a7b3b --no-browser --allow-root;
             "
          ]
          #             jupyter lab --ip=0.0.0.0 --port=8888 --NotebookApp.token=627a7b3b --no-browser --allow-root;
#          Map: 100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 120/120 [00:00<00:00, 1334.24 examples/s]
          #Training dataset size: 170300
          #Validation dataset size: 120
          #Inner training dataset size: 136240
          #Outer training dataset size: 34060
          #Traceback (most recent call last):
          #  File "/DoRA/commonsense_reasoning/finetune.py", line 599, in <module>
          #    fire.Fire(train)
          #  File "/opt/conda/envs/dora_llama/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
          #    component_trace = _Fire(component, args, parsed_flag_args, context, name)
          #  File "/opt/conda/envs/dora_llama/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
          #    component, remaining_args = _CallAndUpdateTrace(
          #  File "/opt/conda/envs/dora_llama/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
          #    component = fn(*varargs, **kwargs)
          #  File "/DoRA/commonsense_reasoning/finetune.py", line 328, in train
          #    args=PEFTTrainingArguments(
          #TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'outer_learning_rate'
          volumeMounts:
            - mountPath: /data
              name: peijia-volume3
          resources:
            requests:
              #          nvidia.com/gpu: "1"
              #          nvidia.com/rtxa6000: "1"
              nvidia.com/a100: "1"
              memory: "12G"
              cpu: "2"
            limits:
              #          nvidia.com/gpu: "1"
              #          nvidia.com/rtxa6000: "1"
              nvidia.com/a100: "1"
              memory: "12G"
              cpu: "2"
      #  affinity:
      #    nodeAffinity:
      #      requiredDuringSchedulingIgnoredDuringExecution:
      #        nodeSelectorTerms:
      #          - matchExpressions:
      #              - key: nvidia.com/gpu.product
      #                operator: In
      #                values:
      #                  - NVIDIA-GeForce-RTX-3090
      volumes:
        - name: peijia-volume3
          persistentVolumeClaim:
            claimName: peijia-volume3
      restartPolicy: Never
